# ADR-0001: ACID table format and storage layout for Bronze/Silver/Gold

**Status:** Proposed  
**Date:** 2026-01-31  
**Owners:** Data Platform (Project)  
**Decision Drivers:** reproducibility, idempotency, schema evolution, PIT features, auditability, Spark-native execution

## Context

This project ingests raw e-commerce data and produces trusted features for churn modeling and API serving. We require:

- **ACID semantics** for upserts/deduplication (Silver), repeatable feature snapshots (Gold), and rerunnable pipelines.
- **Schema evolution** without breaking downstream jobs.
- **Time travel / reproducibility** for debugging and model governance.
- A storage layout that supports **Medallion** boundaries and operational practices (backfills, partition pruning, auditing).

Plain Parquet files alone do not provide transactional guarantees or safe merges. We need an ACID table format on top of Parquet.

## Decision

1. **Adopt Delta Lake tables (Parquet + `_delta_log`) for Bronze, Silver, and Gold.**
2. **Standardize storage layout + naming conventions** to enforce Medallion boundaries and simplify automation.
3. **Use layer-specific write semantics**:
   - **Bronze:** append-only, immutable records (no updates/deletes)
   - **Silver:** upsert/merge allowed (dedupe + integrity)
   - **Gold:** append-only, versioned feature snapshots (PIT-correct), plus a “latest” materialization for serving

## Storage layout

All tables live under a single logical lake root:

lakehouse/
bronze/<source>/<table>/
silver/<domain>/<table>/
gold/<domain>/<table>/


**Examples**
- `lakehouse/bronze/olist/orders/`
- `lakehouse/silver/commerce/orders_clean/`
- `lakehouse/gold/features/customer_churn_features_snapshot/`

## Partitioning standards

- **Bronze:** `ingest_date=YYYY-MM-DD` (always present)
- **Silver:** partition by primary event date where applicable (e.g., `order_date`); otherwise temporarily inherit `ingest_date`
- **Gold:** partition by `as_of_date` (feature snapshot cutoff) for PIT correctness; optionally also partition by `horizon_days` if multiple horizons are supported

## Table identifiers

Code references tables via stable IDs:

- `bronze.<source>.<table>`
- `silver.<domain>.<table>`
- `gold.<domain>.<table>`

Physical locations are resolved from configuration to avoid hardcoding paths.

## Table write patterns

### Bronze

- Append-only writes.
- No dedupe; minimal parsing; preserve raw fidelity.
- Add ingestion metadata columns:
  - `_ingest_ts`, `_ingest_date`, `_source_file`, `_row_hash`, `_schema_hash`

### Silver

- Standardize types, normalize timestamps, enforce keys, dedupe.
- Use `MERGE INTO` for idempotency where keys exist.
- Retain lineage metadata (at minimum `_source_file` and `_ingest_ts`).

### Gold

- Build features using PIT rules: only events `<= as_of_ts` contribute.
- Write **append-only snapshots** keyed by `(customer_id, as_of_date)`.
- Create a separate “latest” serving export step (do not mutate snapshots).

## Options considered

### Option A — Delta Lake (Chosen)

**Pros**
- Strong Spark support: `MERGE INTO`, schema evolution, time travel.
- Mature operational patterns: compaction, vacuum, optimization.
- Fits a “lakehouse-native” narrative cleanly.

**Cons**
- Best experience is Spark-first; cross-engine support varies by environment.
- Requires Delta dependencies and consistent runtime configuration.

### Option B — Apache Iceberg

**Pros**
- Engine-agnostic table format; strong catalog story.
- Clean snapshot semantics.

**Cons**
- More moving parts for merges/catalog in a lean project.
- For Spark-first implementation, Delta is simpler to ship quickly.

### Option C — Plain Parquet only (Rejected)

**Pros**
- Simple file storage.

**Cons**
- No ACID; unsafe upserts/dedupes; brittle schema evolution; weaker reproducibility.

## Consequences

### Positive

- Silver jobs can be **truly idempotent** via merges.
- Gold features can be **recomputed reproducibly** using snapshot partitions.
- Easier debugging via time travel (“what did the table look like yesterday?”).
- Clear Medallion boundary: Bronze preserves raw truth; Silver is trusted; Gold is model-ready.

### Negative / Risks

- Table maintenance is required (compaction/vacuum) or performance degrades over time.
- Delta configs must be consistent across local + CI + cloud runtimes.
- If future requirements demand non-Spark query engines, compatibility must be validated or the format revisited.

## Implementation notes

### Spark configuration

All Spark jobs must enable Delta extensions (set in job bootstrap/config):

- `spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension`
- `spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog`

### Repo mapping

- Ingestion (Bronze): `src/ingestion/`
- Transformations (Silver): `src/transformations/`
- Features (Gold): `src/features/`
- Serving export (latest features): `src/serving_features/`

### Shared IO abstraction

Create a minimal IO layer so format decisions aren’t scattered:

- `src/common/table_io.py`
  - `read_table(table_id, as_of_version=None)`
  - `write_table(table_id, df, mode, partition_cols, merge_keys=None)`

### Maintenance policy (initial)

- **Dev/local:** compact opportunistically; vacuum retention conservative.
- **Stage/prod (later):**
  - scheduled compaction (e.g., weekly)
  - vacuum with retention aligned to rollback needs
  - track table sizes + file counts as operational metrics

## Acceptance criteria

This ADR is “done” when:

1. A Silver job can be rerun without duplicating records (merge/idempotency proven).
2. A Gold feature snapshot can be rebuilt for a given `as_of_date` and match prior outputs.
3. A previous table version can be read for debugging (time travel works in local dev).

## Follow-ups

- ADR-0002: data contracts + schema versioning (must align with table evolution rules)
- ADR-0003: data quality gates (must align with Silver write semantics)
- Add a minimal “table registry” config (table_id → location, partitions, keys)
